# Classification_and_Regression_Models

A hands-on Python project building data preprocessing pipelines, logistic regression, OLS linear & ridge regression, and elastic netâ€”all implemented without scikit-learn or other ML libraries.

## ğŸ” Project Overview
This assignment is structured in five parts using real-world â€œnoisyâ€ datasets:

â€¢ Part I â€“ Data Analysis & Preprocessing  
â€¢ Part II â€“ Logistic Regression via Gradient Descent  
â€¢ Part III â€“ Ordinary Least Squares (OLS) Linear & Ridge Regression  
â€¢ Part IV â€“ Elastic Net Regression via Gradient Descent  
â€¢ Part V - Applying ML methods to a Buffalo-related dataset & boosting penguin accuracy

Each part is delivered as a self-contained Jupyter notebook, complete with plots, pickled weights, and preprocessed CSVs.

## ğŸ¯ Why This Matters
Recruiters and teams value engineers who:  
- Understand data cleaning, imputation, outlier handling, encoding, and normalization  
- Implement core ML algorithms (sigmoid, gradient descent, closed-form OLS) from ground up  
- Tune hyperparameters and regularization (L1, L2, Elastic Net) for robust models  
- Visualize results and interpret metrics (accuracy, MSE, loss curves)  
- Package reproducible code with notebooks, reports, and saved artifacts  

## ğŸ—‚ Table of Contents
1. Quick Start  
2. Notebooks Reference  
3. Architecture & Design  
4. Key Features  
5. Project Structure  
6. Usage Examples  
7. Tech Stack  
8. Future Enhancements  
9. Contributing  
10. License  
11. Authors & Contact  

---

## ğŸ”§ Quick Start
```bash
# 1. Clone repo  
git clone https://github.com/vinaytrinah7755/Classification_and_Regression_Methods.git  
cd Classification_and_Regression_Methods

# 2. Create virtual environment & install deps  
python3 -m venv venv  
source venv/bin/activate  
pip install -r requirements.txt  

# 3. Launch Jupyter  
jupyter notebook  
```

Open and run the notebooks in order:
- penguins_part1.ipynb
- penguins_part2.ipynb
- diamonds_part1.ipynb
- diamonds_part4.ipynb
- emissions_by_country_part1.ipynb
- emissions_by_country_part3.ipynb
- buffalo_covid_part1.ipynb
- buffalo_covid_part2.ipynb

## ğŸ“œ Notebooks Reference

| Notebook | Purpose | Outputs |
|----------|---------|---------|
| *_part1.ipynb | Data cleaning, visualization, feature engineering | *_preprocessed.csv, plots |
| *_part2.ipynb | Binary logistic regression via gradient descent | *_part2_weights.pickle, accuracy |
| *_part3.ipynb | OLS linear regression & ridge regression | *_part3_weights.pickle, MSE |
| *_part4.ipynb | Elastic Net regression via gradient descent | *_part4_weights.pickle, loss |
| bonus.ipynb (optional) | Buffalo dataset modeling & penguin accuracy tuning | bonus weights, results |

## ğŸ— Architecture & Design

- Modular preprocessing functions: handle missing values, string normalization, outlier IQR, encoding, normalization  
- LogisticRegressionGD class: implements sigmoid, cost, gradient descent, fit, predict  
- Closed-form solvers: OLS and ridge using matrix algebra  
- ElasticNetGD class: gradient descent minimizing combined L1/L2 penalty  
- Consistent workflow: load â†’ preprocess â†’ split â†’ train â†’ evaluate â†’ save weights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    preprocess    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    train      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚ raw data â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ cleaned  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ models   â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
```

## ğŸ”‘ Key Features
**Part I â€“ Data Analysis & Preprocessing**  
- Handle missing data (drop/impute), mismatched strings, outliers  
- Encode categorical features (one-hot, label), normalize numerical  
- Generate 5+ insightful visualizations per dataset

**Part II â€“ Logistic Regression**  
- Train on penguin data, tune learning rate & iterations  
- Plot loss curves, achieve >64% accuracy; best â‰ˆ89.9%

**Part III â€“ Linear & Ridge Regression**  
- Solve OLS with closed-form, compare against ridge (L2)  
- Report train/test MSE; ridge reduces overfitting

**Part IV â€“ Elastic Net Regression**  
- Combine L1 & L2 penalties, test zero/random/Xavier init  
- Implement early stopping, compare convergence behaviors

**Bonus Tasks**  
- Apply methods to a Buffalo Open Data dataset (>1k entries)  
- Push penguin classifier beyond 85% with advanced techniques

## ğŸ“ Project Structure
```
Classification_and_Regression_Methods/
â”œâ”€â”€ datasets/  
â”‚   â”œâ”€â”€ penguins.csv  
â”‚   â”œâ”€â”€ diamond.csv 
â”‚   â”œâ”€â”€ emissions_by_country.csv
â”‚   â”œâ”€â”€ buffalo_covid.csv

â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ penguins
â”‚     â”œâ”€â”€ penguins_part1.ipynb  
â”‚     â”œâ”€â”€ penguins_part2.ipynb   
â”‚   â”œâ”€â”€ emissons_by_country
â”‚     â”œâ”€â”€ emissons_by_country_part1.ipynb   
â”‚     â”œâ”€â”€ emissons_by_country_part3.ipynb   
â”‚   â”œâ”€â”€ diamonds
â”‚     â”œâ”€â”€ diamonds_part1.ipynb   
â”‚     â”œâ”€â”€ diamonds_part4.ipynb  
â”‚   â”œâ”€â”€ buffalo_covid
â”‚     â”œâ”€â”€ buffalo_covid_part1.ipynb  
â”‚     â”œâ”€â”€ buffalo_covid_part2.ipynb    
â”œâ”€â”€ outputs/  
â”‚   â”œâ”€â”€ *.csv           # preprocessed datasets  
â”‚   â”œâ”€â”€ *.pickle        # saved model weights  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ LICENSE  
â””â”€â”€ README.md  
```

## ğŸš€ Usage Examples

```python
# In Part II notebook cell:
from logistic import LogisticRegressionGD
model = LogisticRegressionGD(lr=0.004, n_iters=200_000)
model.fit(X_train, y_train)
print("Test Accuracy:", model.score(X_test, y_test))  
# â‡’ ~0.8986
```

```python
# In Part III notebook cell:
from linear import OLS, Ridge
w_ols = OLS().fit(X_train, y_train)
mse_test = OLS().mse(X_test, y_test)
```

## ğŸ›  Tech Stack
- Language: Python 3.8+  
- Core: NumPy, Pandas  
- Visualization: Matplotlib, Seaborn  
- Notebook: Jupyter  
- Pickle: Model weights serialization  

## ğŸŒ± Future Enhancements
- Integrate scikit-learn pipelines for benchmarking  
- Add parallelized/mini-batch gradient descent  
- Hyperparameter search (Grid/Random/Bayesian)  
- Dashboard for real-time metric visualization  

## ğŸ¤ Contributing
1. Fork this repository  
2. Create a feature branch (`git checkout -b feature/xyz`)  
3. Commit your changes (`git commit -m "Add xyz"`)  
4. Push and open a PR for review  

## ğŸ“œ License
This project is licensed under the MIT License. See LICENSE for details.

## ğŸ‘¥ Authors & Contact
**Vinay Trinadh Naraharisetty**    
[GitHub](https://github.com/VinayTrinadh7755)  
[LinkedIn](www.linkedin.com/in/vinay-trinadh-naraharisetty)
  
Thank you for exploring our ML implementations!
